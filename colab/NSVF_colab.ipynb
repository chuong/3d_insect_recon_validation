{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NSVF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNowk740jxBK",
        "outputId": "17b9b61b-2a2f-4fe6-a93f-1a066bd726d6"
      },
      "source": [
        "!git clone --recursive https://github.com/facebookresearch/NSVF.git\n",
        "\n",
        "%cd /content/NSVF\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# %cd NSVF\n",
        "# !pip install --editable ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'NSVF' already exists and is not an empty directory.\n",
            "/content/NSVF\n",
            "Collecting git+https://github.com/MultiPath/fairseq-stable.git (from -r requirements.txt (line 14))\n",
            "  Cloning https://github.com/MultiPath/fairseq-stable.git to /tmp/pip-req-build-c9ec8mzg\n",
            "  Running command git clone -q https://github.com/MultiPath/fairseq-stable.git /tmp/pip-req-build-c9ec8mzg\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/MultiPath/lpips-pytorch.git (from -r requirements.txt (line 15))\n",
            "  Cloning https://github.com/MultiPath/lpips-pytorch.git to /tmp/pip-req-build-71_uqb63\n",
            "  Running command git clone -q https://github.com/MultiPath/lpips-pytorch.git /tmp/pip-req-build-71_uqb63\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: open3d==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.10.0.0)\n",
            "Requirement already satisfied: opencv_python==4.2.0.32 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.2.0.32)\n",
            "Requirement already satisfied: tqdm==4.43.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.43.0)\n",
            "Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.25.3)\n",
            "Requirement already satisfied: imageio==2.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: scikit_image==0.16.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.16.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: plyfile==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: matplotlib==3.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.1.2)\n",
            "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.16.4)\n",
            "Requirement already satisfied: mathutils==2.81.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (2.81.2)\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (2.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 14)) (1.9.0+cu102)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 14)) (0.5.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 14)) (0.29.23)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 14)) (1.14.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 14)) (2019.12.20)\n",
            "Requirement already satisfied: widgetsnbextension in /usr/local/lib/python3.7/dist-packages (from open3d==0.10.0->-r requirements.txt (line 1)) (3.5.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from open3d==0.10.0->-r requirements.txt (line 1)) (7.6.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from open3d==0.10.0->-r requirements.txt (line 1)) (5.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.6.1->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit_image==0.16.2->-r requirements.txt (line 6)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit_image==0.16.2->-r requirements.txt (line 6)) (2.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r requirements.txt (line 9)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r requirements.txt (line 9)) (2.4.7)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 12)) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 12)) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit_image==0.16.2->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.9.0->-r requirements.txt (line 14)) (2.20)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (5.0.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (4.10.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (5.3.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (57.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook->open3d==0.10.0->-r requirements.txt (line 1)) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->open3d==0.10.0->-r requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->open3d==0.10.0->-r requirements.txt (line 1)) (1.7.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.10.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->open3d==0.10.0->-r requirements.txt (line 1)) (22.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook->open3d==0.10.0->-r requirements.txt (line 1)) (21.0)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.9.0->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==0.9.0->-r requirements.txt (line 14)) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCgk3iHz3A34",
        "outputId": "df7c60a0-3eee-4b75-9fea-7ab1075b1860"
      },
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:370: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "building 'fairnr.clib._ext' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/fairnr\n",
            "creating build/temp.linux-x86_64-3.7/fairnr/clib\n",
            "creating build/temp.linux-x86_64-3.7/fairnr/clib/src\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c fairnr/clib/src/binding.cpp -o build/temp.linux-x86_64-3.7/fairnr/clib/src/binding.o -O2 -Ifairnr/clib/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/binding.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            " #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            " \n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c fairnr/clib/src/octree.cpp -o build/temp.linux-x86_64-3.7/fairnr/clib/src/octree.o -O2 -Ifairnr/clib/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/octree.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/octree.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            " #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            " \n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c fairnr/clib/src/intersect.cpp -o build/temp.linux-x86_64-3.7/fairnr/clib/src/intersect.o -O2 -Ifairnr/clib/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            " #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            " \n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor, at::Tensor> ball_intersect(at::Tensor, at::Tensor, at::Tensor, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:23:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_start);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:24:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_dir);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:25:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(points);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor, at::Tensor> aabb_intersect(at::Tensor, at::Tensor, at::Tensor, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:57:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_start);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:58:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_dir);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:59:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(points);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor, at::Tensor> svo_intersect(at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:93:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_start);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:94:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_dir);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:95:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(points);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:96:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(children);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor, at::Tensor> triangle_intersect(at::Tensor, at::Tensor, at::Tensor, float, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:128:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_start);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:129:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(ray_dir);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:130:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(face_points);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/intersect.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/intersect.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c fairnr/clib/src/sample.cpp -o build/temp.linux-x86_64-3.7/fairnr/clib/src/sample.o -O2 -Ifairnr/clib/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            " #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            " \n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor, at::Tensor> uniform_ray_sampling(at::Tensor, at::Tensor, at::Tensor, at::Tensor, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(pts_idx);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:36:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(min_depth);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:37:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(max_depth);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:38:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(uniform_noise);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor, at::Tensor> inverse_cdf_sampling(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:74:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(pts_idx);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(min_depth);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:76:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(max_depth);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:77:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(uniform_noise);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:78:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(probs);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[K’\n",
            " #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K’\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {            \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/include/utils.h:12:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\"); \\\n",
            "     \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:79:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_CUDA(steps);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/include/sample.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairnr/clib/src/sample.cpp:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c fairnr/clib/src/intersect_gpu.cu -o build/temp.linux-x86_64-3.7/fairnr/clib/src/intersect_gpu.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O2 -Ifairnr/clib/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c fairnr/clib/src/sample_gpu.cu -o build/temp.linux-x86_64-3.7/fairnr/clib/src/sample_gpu.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O2 -Ifairnr/clib/include -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/fairnr\n",
            "creating build/lib.linux-x86_64-3.7/fairnr/clib\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairnr/clib/src/binding.o build/temp.linux-x86_64-3.7/fairnr/clib/src/octree.o build/temp.linux-x86_64-3.7/fairnr/clib/src/intersect.o build/temp.linux-x86_64-3.7/fairnr/clib/src/sample.o build/temp.linux-x86_64-3.7/fairnr/clib/src/intersect_gpu.o build/temp.linux-x86_64-3.7/fairnr/clib/src/sample_gpu.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/fairnr/clib/_ext.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/fairnr/clib/_ext.cpython-37m-x86_64-linux-gnu.so -> fairnr/clib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09FvJJXJOEAu",
        "outputId": "377b1070-60ac-43dd-9bfd-83df390d826d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cFOEric6PWw",
        "outputId": "b87d69d5-352b-4e87-e4f3-86ebd9646225"
      },
      "source": [
        "import os\n",
        "os.environ['DATASET'] = \"/content/drive/MyDrive/nsvf/dragonfly\"\n",
        "os.environ['SAVE'] = \"/content/drive/MyDrive/nsvf/dragonfly/data\"\n",
        "# os.environ['DATASET'] = \"/content/drive/MyDrive/Download/Spaceship\"\n",
        "# os.environ['SAVE'] = \"/content/drive/MyDrive/Download\"\n",
        "%cd /content/NSVF\n",
        "\n",
        "!python -u train.py ${DATASET} \\\n",
        "    --user-dir fairnr \\\n",
        "    --task single_object_rendering \\\n",
        "    --train-views \"0..150\" --view-resolution \"1080x1080\" \\\n",
        "    --max-sentences 1 --view-per-batch 2 --pixel-per-view 1024 \\\n",
        "    --no-preload \\\n",
        "    --sampling-on-mask 1.0 --no-sampling-at-reader \\\n",
        "    --valid-views \"150..200\" --valid-view-resolution \"540x540\" \\\n",
        "    --valid-view-per-batch 1 \\\n",
        "    --transparent-background \"1.0,1.0,1.0\" --background-stop-gradient \\\n",
        "    --arch nsvf_base \\\n",
        "    --initial-boundingbox ${DATASET}/bbox.txt \\\n",
        "    --use-octree \\\n",
        "    --raymarching-stepsize-ratio 0.125 \\\n",
        "    --discrete-regularization \\\n",
        "    --color-weight 128.0 --alpha-weight 1.0 \\\n",
        "    --optimizer \"adam\" --adam-betas \"(0.9, 0.999)\" \\\n",
        "    --lr 0.001 --lr-scheduler \"polynomial_decay\" --total-num-update 100000 \\\n",
        "    --criterion \"srn_loss\" --clip-norm 0.0 \\\n",
        "    --num-workers 0 \\\n",
        "    --seed 2 \\\n",
        "    --save-interval-updates 500 --max-update 100000 \\\n",
        "    --virtual-epoch-steps 5000 --save-interval 1 \\\n",
        "    --half-voxel-size-at  \"5000,25000,75000\" \\\n",
        "    --reduce-step-size-at \"5000,25000,75000\" \\\n",
        "    --pruning-every-steps 2500 \\\n",
        "    --keep-interval-updates 5 --keep-last-epochs 5 \\\n",
        "    --log-format simple --log-interval 1 \\\n",
        "    --save-dir ${SAVE} \\\n",
        "    --tensorboard-logdir ${SAVE}/tensorboard \\\n",
        "    | tee -a $SAVE/train.log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314768 GB |  314762 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313587 GB |  313581 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40464 GB |   40452 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40463 GB |   40451 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213549 GB |  213547 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212317 GB |  212315 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   14989    |   14968    |\n",
            "|       from large pool |      11    |      21    |   14815    |   14804    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9881         |        cudaMalloc retries: 9881      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314778 GB |  314772 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313597 GB |  313591 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314778 GB |  314772 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313597 GB |  313591 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40467 GB |   40455 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40467 GB |   40455 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213549 GB |  213547 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212318 GB |  212316 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   14991    |   14970    |\n",
            "|       from large pool |      11    |      21    |   14817    |   14806    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9882         |        cudaMalloc retries: 9882      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314789 GB |  314782 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313608 GB |  313601 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314789 GB |  314782 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313608 GB |  313601 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40471 GB |   40459 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40470 GB |   40458 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213550 GB |  213548 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212318 GB |  212316 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   14993    |   14972    |\n",
            "|       from large pool |      11    |      21    |   14819    |   14808    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9883         |        cudaMalloc retries: 9883      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314799 GB |  314792 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313618 GB |  313611 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314799 GB |  314792 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313618 GB |  313611 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40474 GB |   40462 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40474 GB |   40462 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213551 GB |  213549 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212319 GB |  212317 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32240 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   14995    |   14974    |\n",
            "|       from large pool |      11    |      21    |   14821    |   14810    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9884         |        cudaMalloc retries: 9884      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314809 GB |  314802 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313628 GB |  313621 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314809 GB |  314802 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313628 GB |  313621 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40478 GB |   40466 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40477 GB |   40465 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213551 GB |  213549 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212320 GB |  212318 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   14997    |   14976    |\n",
            "|       from large pool |      11    |      21    |   14823    |   14812    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9885         |        cudaMalloc retries: 9885      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314819 GB |  314813 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313638 GB |  313632 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314819 GB |  314813 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313638 GB |  313632 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40481 GB |   40469 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40481 GB |   40469 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213552 GB |  213550 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212320 GB |  212318 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32240 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   14999    |   14978    |\n",
            "|       from large pool |      11    |      21    |   14825    |   14814    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9886         |        cudaMalloc retries: 9886      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314829 GB |  314823 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313648 GB |  313642 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314829 GB |  314823 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313648 GB |  313642 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40485 GB |   40473 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40484 GB |   40472 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213553 GB |  213551 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212321 GB |  212319 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15001    |   14980    |\n",
            "|       from large pool |      11    |      21    |   14827    |   14816    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9887         |        cudaMalloc retries: 9887      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314839 GB |  314833 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313658 GB |  313652 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314839 GB |  314833 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313658 GB |  313652 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40488 GB |   40476 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40488 GB |   40476 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213553 GB |  213551 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212322 GB |  212319 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15003    |   14982    |\n",
            "|       from large pool |      11    |      21    |   14829    |   14818    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9888         |        cudaMalloc retries: 9888      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314849 GB |  314843 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313669 GB |  313662 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314849 GB |  314843 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313669 GB |  313662 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40492 GB |   40480 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40492 GB |   40480 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213554 GB |  213552 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212322 GB |  212320 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15005    |   14984    |\n",
            "|       from large pool |      11    |      21    |   14831    |   14820    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9889         |        cudaMalloc retries: 9889      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314860 GB |  314853 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313679 GB |  313672 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314860 GB |  314853 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313679 GB |  313672 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40495 GB |   40483 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40495 GB |   40483 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213555 GB |  213552 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212323 GB |  212321 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15007    |   14986    |\n",
            "|       from large pool |      11    |      21    |   14833    |   14822    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9890         |        cudaMalloc retries: 9890      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314870 GB |  314863 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313689 GB |  313682 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314870 GB |  314863 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313689 GB |  313682 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40499 GB |   40487 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40499 GB |   40487 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213555 GB |  213553 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212323 GB |  212321 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15009    |   14988    |\n",
            "|       from large pool |      11    |      21    |   14835    |   14824    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9891         |        cudaMalloc retries: 9891      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314880 GB |  314873 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313699 GB |  313693 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314880 GB |  314873 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313699 GB |  313693 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40502 GB |   40490 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40502 GB |   40490 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213556 GB |  213554 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212324 GB |  212322 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15011    |   14990    |\n",
            "|       from large pool |      11    |      21    |   14837    |   14826    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9892         |        cudaMalloc retries: 9892      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314890 GB |  314884 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313709 GB |  313703 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314890 GB |  314884 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313709 GB |  313703 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40506 GB |   40494 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40506 GB |   40494 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213556 GB |  213554 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212325 GB |  212323 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15013    |   14992    |\n",
            "|       from large pool |      11    |      21    |   14839    |   14828    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9893         |        cudaMalloc retries: 9893      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314900 GB |  314894 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313719 GB |  313713 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314900 GB |  314894 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313719 GB |  313713 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40509 GB |   40497 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40509 GB |   40497 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213557 GB |  213555 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212325 GB |  212323 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15015    |   14994    |\n",
            "|       from large pool |      11    |      21    |   14841    |   14830    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9894         |        cudaMalloc retries: 9894      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314910 GB |  314904 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313729 GB |  313723 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314910 GB |  314904 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313729 GB |  313723 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40513 GB |   40501 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40513 GB |   40501 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213558 GB |  213556 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212326 GB |  212324 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27059 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15017    |   14996    |\n",
            "|       from large pool |      11    |      21    |   14843    |   14832    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2462 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:10:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9895         |        cudaMalloc retries: 9895      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314921 GB |  314914 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313740 GB |  313733 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314921 GB |  314914 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313740 GB |  313733 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40516 GB |   40504 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40516 GB |   40504 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213558 GB |  213556 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212327 GB |  212325 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15019    |   14998    |\n",
            "|       from large pool |      11    |      21    |   14845    |   14834    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2462 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9896         |        cudaMalloc retries: 9896      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314931 GB |  314924 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313750 GB |  313743 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314931 GB |  314924 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313750 GB |  313743 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40520 GB |   40508 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40520 GB |   40508 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213559 GB |  213557 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212327 GB |  212325 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15021    |   15000    |\n",
            "|       from large pool |      11    |      21    |   14847    |   14836    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9897         |        cudaMalloc retries: 9897      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314941 GB |  314934 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313760 GB |  313753 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314941 GB |  314934 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313760 GB |  313753 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40523 GB |   40511 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40523 GB |   40511 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213560 GB |  213558 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212328 GB |  212326 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15023    |   15002    |\n",
            "|       from large pool |      11    |      21    |   14849    |   14838    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9898         |        cudaMalloc retries: 9898      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314951 GB |  314945 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313770 GB |  313764 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314951 GB |  314945 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313770 GB |  313764 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40527 GB |   40515 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40527 GB |   40515 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213560 GB |  213558 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212329 GB |  212327 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32241 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15025    |   15004    |\n",
            "|       from large pool |      11    |      21    |   14851    |   14840    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9899         |        cudaMalloc retries: 9899      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314961 GB |  314955 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313780 GB |  313774 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314961 GB |  314955 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313780 GB |  313774 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40531 GB |   40519 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40530 GB |   40518 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213561 GB |  213559 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212329 GB |  212327 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15027    |   15006    |\n",
            "|       from large pool |      11    |      21    |   14853    |   14842    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19433 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9900         |        cudaMalloc retries: 9900      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314971 GB |  314965 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313790 GB |  313784 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314971 GB |  314965 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313790 GB |  313784 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40534 GB |   40522 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40534 GB |   40522 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213562 GB |  213560 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212330 GB |  212328 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5181 K  |    5181 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15029    |   15008    |\n",
            "|       from large pool |      11    |      21    |   14855    |   14844    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19433 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9901         |        cudaMalloc retries: 9901      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314981 GB |  314975 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313801 GB |  313794 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314981 GB |  314975 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313801 GB |  313794 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40538 GB |   40526 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40537 GB |   40525 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213562 GB |  213560 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212331 GB |  212329 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32241 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15031    |   15010    |\n",
            "|       from large pool |      11    |      21    |   14857    |   14846    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9902         |        cudaMalloc retries: 9902      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  314992 GB |  314985 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313811 GB |  313804 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  314992 GB |  314985 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313811 GB |  313804 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40541 GB |   40529 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40541 GB |   40529 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213563 GB |  213561 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212331 GB |  212329 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15033    |   15012    |\n",
            "|       from large pool |      11    |      21    |   14859    |   14848    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9903         |        cudaMalloc retries: 9903      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315002 GB |  314995 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313821 GB |  313814 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315002 GB |  314995 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313821 GB |  313814 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40545 GB |   40533 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40544 GB |   40532 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213564 GB |  213562 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212332 GB |  212330 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15035    |   15014    |\n",
            "|       from large pool |      11    |      21    |   14861    |   14850    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9904         |        cudaMalloc retries: 9904      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315012 GB |  315005 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313831 GB |  313825 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315012 GB |  315005 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313831 GB |  313825 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40548 GB |   40536 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40548 GB |   40536 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213564 GB |  213562 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212333 GB |  212330 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15037    |   15016    |\n",
            "|       from large pool |      11    |      21    |   14863    |   14852    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9905         |        cudaMalloc retries: 9905      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315022 GB |  315016 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313841 GB |  313835 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315022 GB |  315016 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313841 GB |  313835 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40552 GB |   40540 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40551 GB |   40539 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213565 GB |  213563 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212333 GB |  212331 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15039    |   15018    |\n",
            "|       from large pool |      11    |      21    |   14865    |   14854    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9906         |        cudaMalloc retries: 9906      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315032 GB |  315026 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313851 GB |  313845 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315032 GB |  315026 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313851 GB |  313845 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40555 GB |   40543 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40555 GB |   40543 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213566 GB |  213564 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212334 GB |  212332 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15041    |   15020    |\n",
            "|       from large pool |      11    |      21    |   14867    |   14856    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9907         |        cudaMalloc retries: 9907      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315042 GB |  315036 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313861 GB |  313855 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315042 GB |  315036 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313861 GB |  313855 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40559 GB |   40547 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40558 GB |   40546 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213566 GB |  213564 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212334 GB |  212332 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15043    |   15022    |\n",
            "|       from large pool |      11    |      21    |   14869    |   14858    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9908         |        cudaMalloc retries: 9908      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315053 GB |  315046 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313872 GB |  313865 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315053 GB |  315046 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313872 GB |  313865 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40562 GB |   40550 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40562 GB |   40550 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213567 GB |  213565 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212335 GB |  212333 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15045    |   15024    |\n",
            "|       from large pool |      11    |      21    |   14871    |   14860    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16970 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9909         |        cudaMalloc retries: 9909      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315063 GB |  315056 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313882 GB |  313875 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315063 GB |  315056 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313882 GB |  313875 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40566 GB |   40554 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40565 GB |   40553 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213567 GB |  213565 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212336 GB |  212334 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15047    |   15026    |\n",
            "|       from large pool |      11    |      21    |   14873    |   14862    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9910         |        cudaMalloc retries: 9910      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315073 GB |  315066 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313892 GB |  313885 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315073 GB |  315066 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313892 GB |  313885 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40569 GB |   40557 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40569 GB |   40557 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213568 GB |  213566 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212336 GB |  212334 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15049    |   15028    |\n",
            "|       from large pool |      11    |      21    |   14875    |   14864    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9911         |        cudaMalloc retries: 9911      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315083 GB |  315077 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313902 GB |  313896 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315083 GB |  315077 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313902 GB |  313896 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40573 GB |   40561 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40572 GB |   40560 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213569 GB |  213567 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212337 GB |  212335 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15051    |   15030    |\n",
            "|       from large pool |      11    |      21    |   14877    |   14866    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16970 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9912         |        cudaMalloc retries: 9912      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315093 GB |  315087 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313912 GB |  313906 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315093 GB |  315087 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313912 GB |  313906 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40576 GB |   40564 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40576 GB |   40564 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213569 GB |  213567 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212338 GB |  212336 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15053    |   15032    |\n",
            "|       from large pool |      11    |      21    |   14879    |   14868    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9913         |        cudaMalloc retries: 9913      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315103 GB |  315097 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313922 GB |  313916 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315103 GB |  315097 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313922 GB |  313916 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40580 GB |   40568 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40579 GB |   40567 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213570 GB |  213568 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212338 GB |  212336 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32242 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15055    |   15034    |\n",
            "|       from large pool |      11    |      21    |   14881    |   14870    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9914         |        cudaMalloc retries: 9914      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315113 GB |  315107 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313932 GB |  313926 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315113 GB |  315107 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313932 GB |  313926 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40583 GB |   40571 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40583 GB |   40571 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213571 GB |  213569 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212339 GB |  212337 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15057    |   15036    |\n",
            "|       from large pool |      11    |      21    |   14883    |   14872    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9915         |        cudaMalloc retries: 9915      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315124 GB |  315117 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313943 GB |  313936 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315124 GB |  315117 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313943 GB |  313936 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40587 GB |   40575 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40586 GB |   40574 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213571 GB |  213569 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212340 GB |  212338 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15059    |   15038    |\n",
            "|       from large pool |      11    |      21    |   14885    |   14874    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9916         |        cudaMalloc retries: 9916      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315134 GB |  315127 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313953 GB |  313946 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315134 GB |  315127 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313953 GB |  313946 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40590 GB |   40578 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40590 GB |   40578 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213572 GB |  213570 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212340 GB |  212338 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32242 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15061    |   15040    |\n",
            "|       from large pool |      11    |      21    |   14887    |   14876    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9917         |        cudaMalloc retries: 9917      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315144 GB |  315137 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313963 GB |  313956 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315144 GB |  315137 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313963 GB |  313956 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40594 GB |   40582 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40593 GB |   40581 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213573 GB |  213571 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212341 GB |  212339 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15063    |   15042    |\n",
            "|       from large pool |      11    |      21    |   14889    |   14878    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9918         |        cudaMalloc retries: 9918      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315154 GB |  315148 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313973 GB |  313967 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315154 GB |  315148 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313973 GB |  313967 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40597 GB |   40585 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40597 GB |   40585 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213573 GB |  213571 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212342 GB |  212340 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15065    |   15044    |\n",
            "|       from large pool |      11    |      21    |   14891    |   14880    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9919         |        cudaMalloc retries: 9919      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315164 GB |  315158 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313983 GB |  313977 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315164 GB |  315158 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313983 GB |  313977 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40601 GB |   40589 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40600 GB |   40589 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213574 GB |  213572 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212342 GB |  212340 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15067    |   15046    |\n",
            "|       from large pool |      11    |      21    |   14893    |   14882    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9920         |        cudaMalloc retries: 9920      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315174 GB |  315168 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313993 GB |  313987 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315174 GB |  315168 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  313993 GB |  313987 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40604 GB |   40592 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40604 GB |   40592 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213575 GB |  213573 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212343 GB |  212341 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15069    |   15048    |\n",
            "|       from large pool |      11    |      21    |   14895    |   14884    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9921         |        cudaMalloc retries: 9921      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315185 GB |  315178 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314004 GB |  313997 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315185 GB |  315178 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314004 GB |  313997 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40608 GB |   40596 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40608 GB |   40596 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213575 GB |  213573 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212344 GB |  212342 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15071    |   15050    |\n",
            "|       from large pool |      11    |      21    |   14897    |   14886    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9922         |        cudaMalloc retries: 9922      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315195 GB |  315188 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314014 GB |  314007 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315195 GB |  315188 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314014 GB |  314007 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40611 GB |   40599 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40611 GB |   40599 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213576 GB |  213574 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212344 GB |  212342 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5182 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15073    |   15052    |\n",
            "|       from large pool |      11    |      21    |   14899    |   14888    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9923         |        cudaMalloc retries: 9923      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315205 GB |  315198 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314024 GB |  314017 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315205 GB |  315198 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314024 GB |  314017 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40615 GB |   40603 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40615 GB |   40603 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213577 GB |  213575 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212345 GB |  212343 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5182 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15075    |   15054    |\n",
            "|       from large pool |      11    |      21    |   14901    |   14890    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9924         |        cudaMalloc retries: 9924      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315215 GB |  315209 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314034 GB |  314028 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315215 GB |  315209 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314034 GB |  314028 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40618 GB |   40606 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40618 GB |   40606 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213577 GB |  213575 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212345 GB |  212343 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15077    |   15056    |\n",
            "|       from large pool |      11    |      21    |   14903    |   14892    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9925         |        cudaMalloc retries: 9925      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315225 GB |  315219 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314044 GB |  314038 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315225 GB |  315219 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314044 GB |  314038 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40622 GB |   40610 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40622 GB |   40610 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213578 GB |  213576 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212346 GB |  212344 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15079    |   15058    |\n",
            "|       from large pool |      11    |      21    |   14905    |   14894    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9926         |        cudaMalloc retries: 9926      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315235 GB |  315229 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314054 GB |  314048 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315235 GB |  315229 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314054 GB |  314048 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40625 GB |   40613 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40625 GB |   40613 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213579 GB |  213576 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212347 GB |  212345 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15081    |   15060    |\n",
            "|       from large pool |      11    |      21    |   14907    |   14896    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9927         |        cudaMalloc retries: 9927      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315245 GB |  315239 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314064 GB |  314058 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315245 GB |  315239 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314064 GB |  314058 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40629 GB |   40617 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40629 GB |   40617 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213579 GB |  213577 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212347 GB |  212345 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15083    |   15062    |\n",
            "|       from large pool |      11    |      21    |   14909    |   14898    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9928         |        cudaMalloc retries: 9928      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315256 GB |  315249 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314075 GB |  314068 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315256 GB |  315249 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314075 GB |  314068 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40632 GB |   40620 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40632 GB |   40620 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213580 GB |  213578 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212348 GB |  212346 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32243 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15085    |   15064    |\n",
            "|       from large pool |      11    |      21    |   14911    |   14900    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9929         |        cudaMalloc retries: 9929      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315266 GB |  315259 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314085 GB |  314078 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315266 GB |  315259 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314085 GB |  314078 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40636 GB |   40624 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40636 GB |   40624 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213580 GB |  213578 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212349 GB |  212347 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15087    |   15066    |\n",
            "|       from large pool |      11    |      21    |   14913    |   14902    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9930         |        cudaMalloc retries: 9930      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315276 GB |  315269 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314095 GB |  314088 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315276 GB |  315269 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314095 GB |  314088 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40640 GB |   40627 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40639 GB |   40627 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213581 GB |  213579 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212349 GB |  212347 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15089    |   15068    |\n",
            "|       from large pool |      11    |      21    |   14915    |   14904    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9931         |        cudaMalloc retries: 9931      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315286 GB |  315280 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314105 GB |  314099 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315286 GB |  315280 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314105 GB |  314099 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40643 GB |   40631 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40643 GB |   40631 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213582 GB |  213580 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212350 GB |  212348 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32243 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15091    |   15070    |\n",
            "|       from large pool |      11    |      21    |   14917    |   14906    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9932         |        cudaMalloc retries: 9932      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315296 GB |  315290 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314115 GB |  314109 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315296 GB |  315290 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314115 GB |  314109 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40647 GB |   40635 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40646 GB |   40634 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213582 GB |  213580 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212351 GB |  212349 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15093    |   15072    |\n",
            "|       from large pool |      11    |      21    |   14919    |   14908    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9933         |        cudaMalloc retries: 9933      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315306 GB |  315300 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314125 GB |  314119 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315306 GB |  315300 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314125 GB |  314119 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40650 GB |   40638 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40650 GB |   40638 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213583 GB |  213581 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212351 GB |  212349 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15095    |   15074    |\n",
            "|       from large pool |      11    |      21    |   14921    |   14910    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9934         |        cudaMalloc retries: 9934      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315316 GB |  315310 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314136 GB |  314129 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315316 GB |  315310 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314136 GB |  314129 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40654 GB |   40642 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40653 GB |   40641 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213584 GB |  213582 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212352 GB |  212350 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15097    |   15076    |\n",
            "|       from large pool |      11    |      21    |   14923    |   14912    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9935         |        cudaMalloc retries: 9935      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315327 GB |  315320 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314146 GB |  314139 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315327 GB |  315320 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314146 GB |  314139 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40657 GB |   40645 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40657 GB |   40645 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213584 GB |  213582 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212353 GB |  212351 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15099    |   15078    |\n",
            "|       from large pool |      11    |      21    |   14925    |   14914    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9936         |        cudaMalloc retries: 9936      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315337 GB |  315330 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314156 GB |  314149 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315337 GB |  315330 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314156 GB |  314149 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40661 GB |   40649 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40660 GB |   40648 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213585 GB |  213583 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212353 GB |  212351 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15101    |   15080    |\n",
            "|       from large pool |      11    |      21    |   14927    |   14916    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19434 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9937         |        cudaMalloc retries: 9937      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315347 GB |  315340 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314166 GB |  314160 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315347 GB |  315340 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314166 GB |  314160 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40664 GB |   40652 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40664 GB |   40652 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213586 GB |  213584 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212354 GB |  212352 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15103    |   15082    |\n",
            "|       from large pool |      11    |      21    |   14929    |   14918    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19434 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9938         |        cudaMalloc retries: 9938      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315357 GB |  315351 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314176 GB |  314170 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315357 GB |  315351 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314176 GB |  314170 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40668 GB |   40656 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40667 GB |   40655 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213586 GB |  213584 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212355 GB |  212353 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15105    |   15084    |\n",
            "|       from large pool |      11    |      21    |   14931    |   14920    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9939         |        cudaMalloc retries: 9939      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315367 GB |  315361 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314186 GB |  314180 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315367 GB |  315361 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314186 GB |  314180 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40671 GB |   40659 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40671 GB |   40659 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213587 GB |  213585 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212355 GB |  212353 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15107    |   15086    |\n",
            "|       from large pool |      11    |      21    |   14933    |   14922    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9940         |        cudaMalloc retries: 9940      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315377 GB |  315371 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314196 GB |  314190 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315377 GB |  315371 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314196 GB |  314190 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40675 GB |   40663 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40674 GB |   40662 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213588 GB |  213586 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212356 GB |  212354 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15109    |   15088    |\n",
            "|       from large pool |      11    |      21    |   14935    |   14924    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9941         |        cudaMalloc retries: 9941      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315388 GB |  315381 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314207 GB |  314200 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315388 GB |  315381 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314207 GB |  314200 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40678 GB |   40666 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40678 GB |   40666 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213588 GB |  213586 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212356 GB |  212354 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15111    |   15090    |\n",
            "|       from large pool |      11    |      21    |   14937    |   14926    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9942         |        cudaMalloc retries: 9942      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315398 GB |  315391 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314217 GB |  314210 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315398 GB |  315391 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314217 GB |  314210 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1180 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40682 GB |   40670 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40681 GB |   40669 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213589 GB |  213587 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212357 GB |  212355 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27060 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15113    |   15092    |\n",
            "|       from large pool |      11    |      21    |   14939    |   14928    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9943         |        cudaMalloc retries: 9943      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315408 GB |  315401 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314227 GB |  314220 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315408 GB |  315401 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314227 GB |  314220 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40685 GB |   40673 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40685 GB |   40673 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213590 GB |  213588 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212358 GB |  212356 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15115    |   15094    |\n",
            "|       from large pool |      11    |      21    |   14941    |   14930    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9944         |        cudaMalloc retries: 9944      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315418 GB |  315412 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314237 GB |  314231 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315418 GB |  315412 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314237 GB |  314231 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40689 GB |   40677 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40688 GB |   40676 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213590 GB |  213588 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212358 GB |  212356 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32244 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5183 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15117    |   15096    |\n",
            "|       from large pool |      11    |      21    |   14943    |   14932    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9945         |        cudaMalloc retries: 9945      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315428 GB |  315422 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314247 GB |  314241 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315428 GB |  315422 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314247 GB |  314241 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40692 GB |   40680 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40692 GB |   40680 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213591 GB |  213589 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212359 GB |  212357 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5183 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15119    |   15098    |\n",
            "|       from large pool |      11    |      21    |   14945    |   14934    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9946         |        cudaMalloc retries: 9946      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315438 GB |  315432 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314257 GB |  314251 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315438 GB |  315432 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314257 GB |  314251 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40696 GB |   40684 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40695 GB |   40683 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213592 GB |  213589 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212360 GB |  212358 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32244 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15121    |   15100    |\n",
            "|       from large pool |      11    |      21    |   14947    |   14936    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9947         |        cudaMalloc retries: 9947      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315448 GB |  315442 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314267 GB |  314261 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315448 GB |  315442 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314267 GB |  314261 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40699 GB |   40687 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40699 GB |   40687 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213592 GB |  213590 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212360 GB |  212358 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15123    |   15102    |\n",
            "|       from large pool |      11    |      21    |   14949    |   14938    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9948         |        cudaMalloc retries: 9948      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315459 GB |  315452 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314278 GB |  314271 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315459 GB |  315452 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314278 GB |  314271 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40703 GB |   40691 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40702 GB |   40690 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213593 GB |  213591 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212361 GB |  212359 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27060 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15125    |   15104    |\n",
            "|       from large pool |      11    |      21    |   14951    |   14940    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9949         |        cudaMalloc retries: 9949      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315469 GB |  315462 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314288 GB |  314281 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315469 GB |  315462 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314288 GB |  314281 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40706 GB |   40694 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40706 GB |   40694 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213593 GB |  213591 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212362 GB |  212360 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15127    |   15106    |\n",
            "|       from large pool |      11    |      21    |   14953    |   14942    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9950         |        cudaMalloc retries: 9950      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315479 GB |  315472 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314298 GB |  314291 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315479 GB |  315472 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314298 GB |  314291 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40710 GB |   40698 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40709 GB |   40697 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213594 GB |  213592 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212362 GB |  212360 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15129    |   15108    |\n",
            "|       from large pool |      11    |      21    |   14955    |   14944    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2463 K  |    2463 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9951         |        cudaMalloc retries: 9951      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315489 GB |  315483 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314308 GB |  314302 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315489 GB |  315483 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314308 GB |  314302 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40713 GB |   40701 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40713 GB |   40701 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213595 GB |  213593 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212363 GB |  212361 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15131    |   15110    |\n",
            "|       from large pool |      11    |      21    |   14957    |   14946    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9952         |        cudaMalloc retries: 9952      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315499 GB |  315493 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314318 GB |  314312 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315499 GB |  315493 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314318 GB |  314312 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40717 GB |   40705 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40717 GB |   40705 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213595 GB |  213593 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212364 GB |  212362 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15133    |   15112    |\n",
            "|       from large pool |      11    |      21    |   14959    |   14948    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9953         |        cudaMalloc retries: 9953      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315509 GB |  315503 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314328 GB |  314322 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315509 GB |  315503 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314328 GB |  314322 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40720 GB |   40708 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40720 GB |   40708 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213596 GB |  213594 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212364 GB |  212362 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15135    |   15114    |\n",
            "|       from large pool |      11    |      21    |   14961    |   14950    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9954         |        cudaMalloc retries: 9954      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315520 GB |  315513 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314339 GB |  314332 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315520 GB |  315513 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314339 GB |  314332 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40724 GB |   40712 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40724 GB |   40712 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213597 GB |  213595 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212365 GB |  212363 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15137    |   15116    |\n",
            "|       from large pool |      11    |      21    |   14963    |   14952    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9955         |        cudaMalloc retries: 9955      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315530 GB |  315523 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314349 GB |  314342 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315530 GB |  315523 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314349 GB |  314342 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40727 GB |   40715 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40727 GB |   40715 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213597 GB |  213595 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212366 GB |  212364 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15139    |   15118    |\n",
            "|       from large pool |      11    |      21    |   14965    |   14954    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9956         |        cudaMalloc retries: 9956      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315540 GB |  315533 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314359 GB |  314352 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315540 GB |  315533 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314359 GB |  314352 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40731 GB |   40719 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40731 GB |   40719 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213598 GB |  213596 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212366 GB |  212364 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15141    |   15120    |\n",
            "|       from large pool |      11    |      21    |   14967    |   14956    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9957         |        cudaMalloc retries: 9957      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315550 GB |  315544 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314369 GB |  314363 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315550 GB |  315544 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314369 GB |  314363 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40734 GB |   40722 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40734 GB |   40722 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213599 GB |  213597 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212367 GB |  212365 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15143    |   15122    |\n",
            "|       from large pool |      11    |      21    |   14969    |   14958    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9958         |        cudaMalloc retries: 9958      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315560 GB |  315554 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314379 GB |  314373 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315560 GB |  315554 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314379 GB |  314373 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40738 GB |   40726 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40738 GB |   40726 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213599 GB |  213597 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212367 GB |  212365 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15145    |   15124    |\n",
            "|       from large pool |      11    |      21    |   14971    |   14960    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9959         |        cudaMalloc retries: 9959      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315570 GB |  315564 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314389 GB |  314383 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315570 GB |  315564 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314389 GB |  314383 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40741 GB |   40729 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40741 GB |   40729 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213600 GB |  213598 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212368 GB |  212366 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32245 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15147    |   15126    |\n",
            "|       from large pool |      11    |      21    |   14973    |   14962    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9960         |        cudaMalloc retries: 9960      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315580 GB |  315574 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314399 GB |  314393 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315580 GB |  315574 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314399 GB |  314393 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40745 GB |   40733 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40745 GB |   40733 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213601 GB |  213599 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212369 GB |  212367 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15149    |   15128    |\n",
            "|       from large pool |      11    |      21    |   14975    |   14964    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9961         |        cudaMalloc retries: 9961      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315591 GB |  315584 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314410 GB |  314403 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315591 GB |  315584 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314410 GB |  314403 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40748 GB |   40736 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40748 GB |   40736 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213601 GB |  213599 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212369 GB |  212367 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32245 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15151    |   15130    |\n",
            "|       from large pool |      11    |      21    |   14977    |   14966    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9962         |        cudaMalloc retries: 9962      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315601 GB |  315594 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314420 GB |  314413 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315601 GB |  315594 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314420 GB |  314413 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40752 GB |   40740 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40752 GB |   40740 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213602 GB |  213600 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212370 GB |  212368 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15153    |   15132    |\n",
            "|       from large pool |      11    |      21    |   14979    |   14968    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9963         |        cudaMalloc retries: 9963      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315611 GB |  315604 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314430 GB |  314423 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315611 GB |  315604 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314430 GB |  314423 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40756 GB |   40744 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40755 GB |   40743 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213603 GB |  213600 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212371 GB |  212369 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15155    |   15134    |\n",
            "|       from large pool |      11    |      21    |   14981    |   14970    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9964         |        cudaMalloc retries: 9964      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315621 GB |  315615 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314440 GB |  314434 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315621 GB |  315615 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314440 GB |  314434 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40759 GB |   40747 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40759 GB |   40747 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213603 GB |  213601 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212371 GB |  212369 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15157    |   15136    |\n",
            "|       from large pool |      11    |      21    |   14983    |   14972    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9965         |        cudaMalloc retries: 9965      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315631 GB |  315625 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314450 GB |  314444 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315631 GB |  315625 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314450 GB |  314444 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40763 GB |   40751 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40762 GB |   40750 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213604 GB |  213602 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212372 GB |  212370 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15159    |   15138    |\n",
            "|       from large pool |      11    |      21    |   14985    |   14974    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9966         |        cudaMalloc retries: 9966      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315641 GB |  315635 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314460 GB |  314454 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315641 GB |  315635 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314460 GB |  314454 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40766 GB |   40754 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40766 GB |   40754 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213604 GB |  213602 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212373 GB |  212371 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15161    |   15140    |\n",
            "|       from large pool |      11    |      21    |   14987    |   14976    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9967         |        cudaMalloc retries: 9967      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315652 GB |  315645 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314471 GB |  314464 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315652 GB |  315645 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314471 GB |  314464 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40770 GB |   40758 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40769 GB |   40757 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213605 GB |  213603 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212373 GB |  212371 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5184 K  |    5184 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15163    |   15142    |\n",
            "|       from large pool |      11    |      21    |   14989    |   14978    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9968         |        cudaMalloc retries: 9968      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315662 GB |  315655 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314481 GB |  314474 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315662 GB |  315655 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314481 GB |  314474 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40773 GB |   40761 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40773 GB |   40761 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213606 GB |  213604 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212374 GB |  212372 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15165    |   15144    |\n",
            "|       from large pool |      11    |      21    |   14991    |   14980    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9969         |        cudaMalloc retries: 9969      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315672 GB |  315665 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314491 GB |  314484 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315672 GB |  315665 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314491 GB |  314484 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40777 GB |   40765 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40776 GB |   40764 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213606 GB |  213604 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212375 GB |  212373 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15167    |   15146    |\n",
            "|       from large pool |      11    |      21    |   14993    |   14982    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9970         |        cudaMalloc retries: 9970      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315682 GB |  315676 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314501 GB |  314495 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315682 GB |  315676 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314501 GB |  314495 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40780 GB |   40768 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40780 GB |   40768 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213607 GB |  213605 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212375 GB |  212373 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15169    |   15148    |\n",
            "|       from large pool |      11    |      21    |   14995    |   14984    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9971         |        cudaMalloc retries: 9971      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315692 GB |  315686 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314511 GB |  314505 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315692 GB |  315686 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314511 GB |  314505 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40784 GB |   40772 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40783 GB |   40771 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213608 GB |  213606 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212376 GB |  212374 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15171    |   15150    |\n",
            "|       from large pool |      11    |      21    |   14997    |   14986    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9972         |        cudaMalloc retries: 9972      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315702 GB |  315696 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314521 GB |  314515 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315702 GB |  315696 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314521 GB |  314515 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40787 GB |   40775 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40787 GB |   40775 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213608 GB |  213606 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212377 GB |  212375 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15173    |   15152    |\n",
            "|       from large pool |      11    |      21    |   14999    |   14988    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9973         |        cudaMalloc retries: 9973      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315712 GB |  315706 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314531 GB |  314525 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315712 GB |  315706 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314531 GB |  314525 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40791 GB |   40779 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40790 GB |   40778 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213609 GB |  213607 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212377 GB |  212375 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15175    |   15154    |\n",
            "|       from large pool |      11    |      21    |   15001    |   14990    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19435 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9974         |        cudaMalloc retries: 9974      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315723 GB |  315716 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314542 GB |  314535 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315723 GB |  315716 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314542 GB |  314535 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40794 GB |   40782 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40794 GB |   40782 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213610 GB |  213608 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212378 GB |  212376 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32246 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15177    |   15156    |\n",
            "|       from large pool |      11    |      21    |   15003    |   14992    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19435 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9975         |        cudaMalloc retries: 9975      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315733 GB |  315726 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314552 GB |  314545 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315733 GB |  315726 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314552 GB |  314545 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40798 GB |   40786 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40797 GB |   40785 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213610 GB |  213608 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212378 GB |  212376 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15179    |   15158    |\n",
            "|       from large pool |      11    |      21    |   15005    |   14994    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9976         |        cudaMalloc retries: 9976      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315743 GB |  315736 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314562 GB |  314555 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315743 GB |  315736 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314562 GB |  314555 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40801 GB |   40789 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40801 GB |   40789 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213611 GB |  213609 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212379 GB |  212377 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32246 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15181    |   15160    |\n",
            "|       from large pool |      11    |      21    |   15007    |   14996    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9977         |        cudaMalloc retries: 9977      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315753 GB |  315747 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314572 GB |  314566 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315753 GB |  315747 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314572 GB |  314566 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40805 GB |   40793 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40804 GB |   40792 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213612 GB |  213610 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212380 GB |  212378 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15183    |   15162    |\n",
            "|       from large pool |      11    |      21    |   15009    |   14998    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9978         |        cudaMalloc retries: 9978      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315763 GB |  315757 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314582 GB |  314576 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315763 GB |  315757 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314582 GB |  314576 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40808 GB |   40796 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40808 GB |   40796 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213612 GB |  213610 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212380 GB |  212378 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15185    |   15164    |\n",
            "|       from large pool |      11    |      21    |   15011    |   15000    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9979         |        cudaMalloc retries: 9979      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315773 GB |  315767 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314592 GB |  314586 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315773 GB |  315767 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314592 GB |  314586 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40812 GB |   40800 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40811 GB |   40799 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213613 GB |  213611 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212381 GB |  212379 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15187    |   15166    |\n",
            "|       from large pool |      11    |      21    |   15013    |   15002    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9980         |        cudaMalloc retries: 9980      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315784 GB |  315777 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314602 GB |  314596 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315784 GB |  315777 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314602 GB |  314596 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40815 GB |   40803 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40815 GB |   40803 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213614 GB |  213612 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212382 GB |  212380 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15189    |   15168    |\n",
            "|       from large pool |      11    |      21    |   15015    |   15004    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9981         |        cudaMalloc retries: 9981      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315794 GB |  315787 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314613 GB |  314606 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315794 GB |  315787 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314613 GB |  314606 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40819 GB |   40807 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40818 GB |   40806 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213614 GB |  213612 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212382 GB |  212380 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15191    |   15170    |\n",
            "|       from large pool |      11    |      21    |   15017    |   15006    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9982         |        cudaMalloc retries: 9982      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315804 GB |  315797 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314623 GB |  314616 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315804 GB |  315797 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314623 GB |  314616 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40822 GB |   40810 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40822 GB |   40810 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213615 GB |  213613 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212383 GB |  212381 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15193    |   15172    |\n",
            "|       from large pool |      11    |      21    |   15019    |   15008    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9983         |        cudaMalloc retries: 9983      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315814 GB |  315808 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314633 GB |  314626 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315814 GB |  315808 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314633 GB |  314626 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40826 GB |   40814 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40825 GB |   40814 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213616 GB |  213613 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212384 GB |  212382 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15195    |   15174    |\n",
            "|       from large pool |      11    |      21    |   15021    |   15010    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9984         |        cudaMalloc retries: 9984      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315824 GB |  315818 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314643 GB |  314637 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315824 GB |  315818 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314643 GB |  314637 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40829 GB |   40817 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40829 GB |   40817 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213616 GB |  213614 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212384 GB |  212382 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15197    |   15176    |\n",
            "|       from large pool |      11    |      21    |   15023    |   15012    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9985         |        cudaMalloc retries: 9985      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315834 GB |  315828 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314653 GB |  314647 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315834 GB |  315828 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314653 GB |  314647 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40833 GB |   40821 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40833 GB |   40821 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213617 GB |  213615 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212385 GB |  212383 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15199    |   15178    |\n",
            "|       from large pool |      11    |      21    |   15025    |   15014    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9986         |        cudaMalloc retries: 9986      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315844 GB |  315838 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314663 GB |  314657 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315844 GB |  315838 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314663 GB |  314657 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40836 GB |   40824 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40836 GB |   40824 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213617 GB |  213615 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212386 GB |  212384 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15201    |   15180    |\n",
            "|       from large pool |      11    |      21    |   15027    |   15016    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9987         |        cudaMalloc retries: 9987      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315855 GB |  315848 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314674 GB |  314667 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315855 GB |  315848 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314674 GB |  314667 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40840 GB |   40828 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40840 GB |   40828 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213618 GB |  213616 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212386 GB |  212384 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15203    |   15182    |\n",
            "|       from large pool |      11    |      21    |   15029    |   15018    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9988         |        cudaMalloc retries: 9988      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315865 GB |  315858 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314684 GB |  314677 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315865 GB |  315858 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314684 GB |  314677 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40843 GB |   40831 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40843 GB |   40831 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213619 GB |  213617 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212387 GB |  212385 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15205    |   15184    |\n",
            "|       from large pool |      11    |      21    |   15031    |   15020    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9989         |        cudaMalloc retries: 9989      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315875 GB |  315868 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314694 GB |  314687 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315875 GB |  315868 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314694 GB |  314687 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40847 GB |   40835 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40847 GB |   40835 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213619 GB |  213617 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212388 GB |  212386 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32247 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5185 K  |    5185 K  |\n",
            "|       from small pool |     137    |   37373    |   27061 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15207    |   15186    |\n",
            "|       from large pool |      11    |      21    |   15033    |   15022    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9990         |        cudaMalloc retries: 9990      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315885 GB |  315879 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314704 GB |  314698 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315885 GB |  315879 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314704 GB |  314698 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40850 GB |   40838 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40850 GB |   40838 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213620 GB |  213618 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212388 GB |  212386 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15209    |   15188    |\n",
            "|       from large pool |      11    |      21    |   15035    |   15024    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9991         |        cudaMalloc retries: 9991      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315895 GB |  315889 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314714 GB |  314708 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315895 GB |  315889 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314714 GB |  314708 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40854 GB |   40842 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40854 GB |   40842 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213621 GB |  213619 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212389 GB |  212387 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32247 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15211    |   15190    |\n",
            "|       from large pool |      11    |      21    |   15037    |   15026    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9992         |        cudaMalloc retries: 9992      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315905 GB |  315899 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314724 GB |  314718 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315905 GB |  315899 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314724 GB |  314718 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40857 GB |   40845 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40857 GB |   40845 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213621 GB |  213619 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212389 GB |  212387 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15213    |   15192    |\n",
            "|       from large pool |      11    |      21    |   15039    |   15028    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9993         |        cudaMalloc retries: 9993      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315916 GB |  315909 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314734 GB |  314728 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315916 GB |  315909 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314734 GB |  314728 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40861 GB |   40849 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40861 GB |   40849 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213622 GB |  213620 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212390 GB |  212388 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15215    |   15194    |\n",
            "|       from large pool |      11    |      21    |   15041    |   15030    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9994         |        cudaMalloc retries: 9994      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315926 GB |  315919 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314745 GB |  314738 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315926 GB |  315919 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314745 GB |  314738 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40865 GB |   40852 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40864 GB |   40852 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213623 GB |  213621 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212391 GB |  212389 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15217    |   15196    |\n",
            "|       from large pool |      11    |      21    |   15043    |   15032    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9995         |        cudaMalloc retries: 9995      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315936 GB |  315929 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314755 GB |  314748 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315936 GB |  315929 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314755 GB |  314748 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40868 GB |   40856 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40868 GB |   40856 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213623 GB |  213621 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212391 GB |  212389 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15219    |   15198    |\n",
            "|       from large pool |      11    |      21    |   15045    |   15034    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9996         |        cudaMalloc retries: 9996      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315946 GB |  315940 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314765 GB |  314758 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315946 GB |  315940 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314765 GB |  314758 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40872 GB |   40860 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40871 GB |   40859 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213624 GB |  213622 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212392 GB |  212390 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27061 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15221    |   15200    |\n",
            "|       from large pool |      11    |      21    |   15047    |   15036    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9997         |        cudaMalloc retries: 9997      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315956 GB |  315950 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314775 GB |  314769 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315956 GB |  315950 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314775 GB |  314769 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40875 GB |   40863 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40875 GB |   40863 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213625 GB |  213623 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212393 GB |  212391 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27062 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27062 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15223    |   15202    |\n",
            "|       from large pool |      11    |      21    |   15049    |   15038    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2021-07-27 19:11:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.76 GiB total capacity; 9.97 GiB already allocated; 1.65 GiB free; 12.01 GiB reserved in total by PyTorch)\n",
            "2021-07-27 19:11:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9998         |        cudaMalloc retries: 9998      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6614 MB |   12046 MB |  315966 GB |  315960 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314785 GB |  314779 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6614 MB |   12046 MB |  315966 GB |  315960 GB |\n",
            "|       from large pool |    6605 MB |   12037 MB |  314785 GB |  314779 GB |\n",
            "|       from small pool |       8 MB |      77 MB |    1181 GB |    1181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12300 MB |   13304 MB |   40879 GB |   40867 GB |\n",
            "|       from large pool |   12280 MB |   13222 MB |   40878 GB |   40866 GB |\n",
            "|       from small pool |      20 MB |      82 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2085 MB |    5845 MB |  213625 GB |  213623 GB |\n",
            "|       from large pool |    2074 MB |    5835 MB |  212393 GB |  212391 GB |\n",
            "|       from small pool |      11 MB |      30 MB |    1231 GB |    1231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27062 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     164    |   37396    |   32248 K  |   32248 K  |\n",
            "|       from large pool |      27    |     132    |    5186 K  |    5186 K  |\n",
            "|       from small pool |     137    |   37373    |   27062 K  |   27062 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      21    |      61    |   15225    |   15204    |\n",
            "|       from large pool |      11    |      21    |   15051    |   15040    |\n",
            "|       from small pool |      10    |      41    |     174    |     164    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |    1009    |   19436 K  |   19436 K  |\n",
            "|       from large pool |      14    |      25    |    2464 K  |    2464 K  |\n",
            "|       from small pool |      29    |     998    |   16971 K  |   16971 K  |\n",
            "|===========================================================================|\n",
            "\n",
            "2021-07-27 19:11:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 20, in <module>\n",
            "    cli_main()\n",
            "  File \"/content/NSVF/fairnr_cli/train.py\", line 373, in cli_main\n",
            "    main(args)\n",
            "  File \"/content/NSVF/fairnr_cli/train.py\", line 104, in main\n",
            "    should_end_training = train(args, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/NSVF/fairnr_cli/train.py\", line 211, in train\n",
            "    progress.print(stats, tag='train', step=num_updates)\n",
            "UnboundLocalError: local variable 'num_updates' referenced before assignment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUU-ZhpRZdBf"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/nsvf/ac_nsvf/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMlKZgDE3XHJ"
      },
      "source": [
        "# 3D reconstraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGzCs25XT1UF",
        "outputId": "8c7bb6a9-d4b6-4220-fdac-e0e19477b4d8"
      },
      "source": [
        "%cd /content/NSVF\n",
        "import os\n",
        "# os.environ['MODEL_PATH'] = \"/content/drive/MyDrive/Download/checkpoint_last.pt\"\n",
        "# os.environ['SAVE'] = \"/content/drive/MyDrive/Download/Spaceship\"\n",
        "os.environ['MODEL_PATH'] = \"/content/drive/MyDrive/nsvf/dragonfly/data/checkpoint_last.pt\"\n",
        "os.environ['SAVE'] = \"/content/drive/MyDrive/nsvf/dragonfly/\"\n",
        "os.environ['NAME'] = \"dragonfly\"\n",
        "!python extract.py \\\n",
        "    --user-dir fairnr \\\n",
        "    --path ${MODEL_PATH} \\\n",
        "    --output ${SAVE} \\\n",
        "    --name ${NAME} \\\n",
        "    --format 'mc_mesh' \\\n",
        "    --mc-threshold 0.6 \\\n",
        "    --mc-num-samples-per-halfvoxel 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NSVF\n",
            "2021-07-29 12:15:56 | INFO | fairnr_cli.extract | Namespace(bf16=False, cpu=False, format='mc_mesh', mc_num_samples_per_halfvoxel=5, mc_threshold=0.6, name='dragonfly', no_seed_provided=True, output='/content/drive/MyDrive/nsvf/dragonfly/', path='/content/drive/MyDrive/nsvf/dragonfly/data/checkpoint_last.pt', savetext=False, seed=1, tpu=False, user_dir='fairnr')\n",
            "2021-07-29 12:15:59 | INFO | fairnr.modules.encoder | loaded 3969 voxel centers, 4840 voxel corners\n",
            "2021-07-29 12:16:05 | INFO | fairnr.modules.encoder | marching cube...\n",
            "2021-07-29 12:16:05 | INFO | fairnr.modules.encoder | reset chache\n",
            "Building EasyOctree done. total #nodes = 67703, terminal #nodes = 30488 (time taken 15.304386 s)\n",
            "tcmalloc: large alloc 3015680000 bytes == 0x55ac9411a000 @  0x7f5c3e0cfb6b 0x7f5c3e0ef379 0x7f5bd763626e 0x7f5bd76379e2 0x7f5c1bd52a73 0x7f5c1b288b7b 0x7f5c1b9e3bef 0x7f5c1b9c8480 0x7f5c1b5d1454 0x7f5c1b27c1c8 0x7f5c1bb56310 0x7f5c1bd21217 0x7f5c2dc999ae 0x7f5c2dde59de 0x55ac20fb8010 0x55ac210a9c0d 0x55ac2102c0d8 0x55ac21026c35 0x55ac20ef8e2c 0x55ac21029318 0x55ac21027235 0x55ac20fb973a 0x55ac2102893b 0x55ac20fb965a 0x55ac21027b0e 0x55ac20fb965a 0x55ac21027b0e 0x55ac21026c35 0x55ac21026933 0x55ac210f0402 0x55ac210f077d\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}